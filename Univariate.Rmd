---
title: "Univariate"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(car)
```

Split the train data into train and validation. The last 2 years form a part of the validation set.
```{r}
train = read.csv('train.csv')
train = na.omit(train)
train = log(train)
valid = train[265:288,]
train = train[1:264,]

valid = valid$Bankruptcy_Rate
valid = ts(valid, start = c(2009,1), frequency = 12 )
valid
```

```{r}
only_bankruptcy = ts(train$Bankruptcy_Rate, start = c(1987,1), frequency = 12)
plot(only_bankruptcy)
adf.test(only_bankruptcy)
d = 0
pvalue = 1
while(pvalue > 0.05){
  d = d+1  
  pvalue = adf.test(diff(only_bankruptcy, differences = d))$p.value
}
bankruptcy = diff(only_bankruptcy, differences = d)
print(d)
print(pvalue)
```



```{r}
plot(bankruptcy)
plot(only_bankruptcy)
acf(bankruptcy)
pacf(bankruptcy)

```
We suspect this could be a MA(1) or AR(2).
```{r}
# Fit AR(p) Models:
ar1 <- arima(bankruptcy, order=c(1,0,0)) #AR(1)
ar2 <- arima(bankruptcy, order=c(2,0,0)) #AR(2)
ar3 <- arima(bankruptcy, order=c(3,0,0)) #AR(3)
sigma2<-c(ar1$sigma2,ar2$sigma2,ar3$sigma2)
loglik<-c(ar1$loglik,ar2$loglik,ar3$loglik)
AIC<-c(ar1$aic,ar2$aic,ar3$aic)
d <- data.frame(pq = c("(1,0)","(2,0)","(3,0)"),sigma2,loglik,AIC)
d

```
From the table, we can see that AR(3) seems better because the sigma-squared and AIC values are lesser as compared to AR(1) and AR(2) while the log likelihood is higher.



```{r}
# Fit AR(p) Models:
ma1 <- arima(bankruptcy, order=c(0,0,1)) #MA(1)
ma2 <- arima(bankruptcy, order=c(0,0,2)) #MA(2)
ma3 <- arima(bankruptcy, order=c(0,0,3)) #MA(3)
sigma2<-c(ma1$sigma2,ma2$sigma2,ma3$sigma2)
loglik<-c(ma1$loglik,ma2$loglik,ma3$loglik)
AIC<-c(ma1$aic,ma2$aic,ma3$aic)
d <- data.frame(pq = c("(0,1)","(0,2)","(0,3)"),sigma2,loglik,AIC)
d

```
Here, we could say that MA(2) fits better than MA(1) and MA(3) because of the sigma - squared and AIC values.

```{r}

arma1_1 <- arima(bankruptcy, order=c(1,0,1)) 
arma1_2 <- arima(bankruptcy, order=c(1,0,2)) 
arma1_3 <- arima(bankruptcy, order=c(1,0,3))
arma2_1 <- arima(bankruptcy, order=c(2,0,1)) 
arma2_2 <- arima(bankruptcy, order=c(2,0,2)) 
arma2_3 <- arima(bankruptcy, order=c(2,0,3)) 
arma3_1 <- arima(bankruptcy, order=c(3,0,1)) 
arma3_2 <- arima(bankruptcy, order=c(3,0,2)) 
arma3_3 <- arima(bankruptcy, order=c(3,0,3)) 
sigma2<-c(arma1_1$sigma2,arma1_2$sigma2,arma1_3$sigma2, arma2_1$sigma2, arma2_2$sigma2, arma2_3$sigma2, arma3_1$sigma2, arma3_2$sigma2, arma3_3$sigma2)
loglik<-c(arma1_1$loglik,arma1_2$loglik,arma1_3$loglik, arma2_1$loglik, arma2_2$loglik, arma2_3$loglik, arma3_1$loglik, arma3_2$loglik, arma3_3$loglik)
AIC<-c(arma1_1$aic,arma1_2$aic,arma1_3$aic, arma2_1$aic, arma2_2$aic, arma2_3$aic, arma3_1$aic, arma3_2$aic, arma3_3$aic)
d <- data.frame(pq = c("(1,1)","(1,2)","(1,3)", "(2,1)", '(2,2)', '(2,3)', '(3,1)','(3,2)','(3,3)'),sigma2,loglik,AIC)
d



```

Thus this could be an ARIMA(3,1,3) process.

But we also see seasonality in the data, to remove which we difference using period = 12
```{r}
bankruptcy <- diff(bankruptcy, lag = 12)
plot(bankruptcy)
acf(bankruptcy, lag.max = 48)
pacf(bankruptcy, lag.max = 100)
```

```{r}
auto.arima(only_bankruptcy,d = 1, D=1)
a = arima(only_bankruptcy, c(2,1,3), c(2,1,2))
a_stat = c(a$sigma2, a$loglik, a$aic)

b = arima(only_bankruptcy, c(2,1,2), c(2,1,2))
b_stat = c(b$sigma2, b$loglik, b$aic)

c = arima(only_bankruptcy, c(2,1,2), c(2,1,1))
c_stat = c(c$sigma2, c$loglik, c$aic)

d = arima(only_bankruptcy, c(2,1,3), c(2,1,1))
d_stat = c(d$sigma2, d$loglik, d$aic)

e = arima(only_bankruptcy, c(0,1,3), c(2,1,3))
e_stat = c(e$sigma2, e$loglik, e$aic)

g = arima(only_bankruptcy, c(1,1,3), c(2,1,3))
g_stat = c(g$sigma2, g$loglik, g$aic)

h = arima(only_bankruptcy, c(2,1,3), c(2,1,3))
h_stat = c(h$sigma2, h$loglik, h$aic)





table1 = rbind(a_stat, b_stat, c_stat, d_stat, e_stat, g_stat, h_stat)
colnames(table1) = c('Sigma2', 'loglikelihood', 'AIC')
table1
```

###Likelihood Ratio Test:
```{r}
D <- -2*(e$loglik - h$loglik)
pval <- 1-pchisq(D,2)
print(c("Test Statistic:",round(D, 4),"P-value:", round(pval, 4)))

```


From this we can see that SARIMA(0,1,3)(2,1,3)[12] does better than the other models. Since SARIMA works better than any of the previous methods (from the Box Jenkins approach), we focus our analysis on SARIMA. For checking which model is better, we first do a residual diagnostic to check if our model is a good fit on the data.


```{r}
# test whether residuals have zero mean
(t.test(e$residuals))
plot(e$residuals, main = "Residuals vs. Time", ylab = "Residuals")
abline(h = 0, col = "red")
```

Test for Zero-Correlation:
```{r fig.height=5}
tsdiag(e)
```
Test for homoscedasticity
```{r}
group <- c(rep(1,60),rep(2,60),rep(3,60),rep(4,60),rep(5,24))


(leveneTest(e$residuals,group)) #Levene
#(bartlett.test(g$residuals,group)) #Bartlett  
scatter.smooth(e$residuals, ylab = 'Residuals')
```
Test for normality:
```{r}
qqnorm(e$residuals)
qqline(e$residuals, col = "red")
(shapiro.test(e$residuals))
```
All tests except normality are passed due to outliers at the beginning of the time series.

**Testing on the validation set:**
```{r}

forecast_sarima = forecast(e, h = 72, level = 95)
plot(forecast_sarima)
valid
#forecast_sarima
rmse_sarima = rmse(valid, forecast_sarima$mean)
rmse_sarima
```





##Double Exponential Smoothing
```{r}
hw.CH <- HoltWinters(x = only_bankruptcy, gamma = F) 
par(mfrow = c(2,1))
plot(hw.CH)

```

##Triple Exponential Smoothing - Additive
```{r}
hw.AD <- HoltWinters(x = only_bankruptcy, seasonal = "add") 
par(mfrow = c(2,1))
plot(hw.AD)

```

##Triple Exponential Smoothing - Multiplicative
```{r}
hw.AD <- HoltWinters(x = only_bankruptcy, seasonal = "multiplicative") 
par(mfrow = c(2,1))
plot(hw.AD)

```